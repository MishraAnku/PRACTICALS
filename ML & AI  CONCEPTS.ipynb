{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[ \\]:\n",
    "\n",
    "    # machine learning \n",
    "    # warnings - this means the letest pythhon is not installed so thats why warning message is coming .\n",
    "    # ignore - it is used to ignore te warnings \n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    "    # 1st we have impoted libraries \n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt \n",
    "    import seaborn as sns\n",
    "\n",
    "    import warnings \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    # checking for nan values \n",
    "\n",
    "    np.nan\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "    nan\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    # handling missing values \n",
    "\n",
    "    name=[\"john\",\"jack\",\"jhill\",\"kenny\",\"kelvin\"]\n",
    "    mat=[58,85,47,np.nan,89]\n",
    "    science=[78,58,np.nan,np.nan,79]\n",
    "\n",
    "In \\[39\\]:\n",
    "\n",
    "    import math\n",
    "\n",
    "In \\[40\\]:\n",
    "\n",
    "    # creating dictionary \n",
    "\n",
    "    dic={\"name\":name,\"math\":math,\"science\":science }\n",
    "\n",
    "In \\[41\\]:\n",
    "\n",
    "    dic\n",
    "\n",
    "Out\\[41\\]:\n",
    "\n",
    "    {'name': ['john', 'jack', 'jhill', 'kenny', 'kelvin'],\n",
    "     'math': <module 'math' (built-in)>,\n",
    "     'science': [78, 58, nan, nan, 79]}\n",
    "\n",
    "In \\[42\\]:\n",
    "\n",
    "    # stored dictionary into the dataframe \n",
    "\n",
    "    df=pd.DataFrame(dic)\n",
    "\n",
    "In \\[43\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[43\\]:\n",
    "\n",
    "|     | name   | math                        | science |\n",
    "|-----|--------|-----------------------------|---------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> | 78.0    |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> | 58.0    |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> | NaN     |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> | NaN     |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> | 79.0    |\n",
    "\n",
    "In \\[44\\]:\n",
    "\n",
    "    # before we feed te data into thhe machine \n",
    "    # machine does not understand the missing values so remove missing values\n",
    "    # skewness to be removed \n",
    "    # machine do not understand te categorical data so handle the categorical data carefully \n",
    "\n",
    "In \\[45\\]:\n",
    "\n",
    "    # drop null values \n",
    "\n",
    "    df.isnull()\n",
    "\n",
    "Out\\[45\\]:\n",
    "\n",
    "|     | name  | math  | science |\n",
    "|-----|-------|-------|---------|\n",
    "| 0   | False | False | False   |\n",
    "| 1   | False | False | False   |\n",
    "| 2   | False | False | True    |\n",
    "| 3   | False | False | True    |\n",
    "| 4   | False | False | False   |\n",
    "\n",
    "In \\[37\\]:\n",
    "\n",
    "    # dropna is used to remove null values \n",
    "\n",
    "    df.dropna()\n",
    "\n",
    "Out\\[37\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    # it will remove the null values from columns \n",
    "\n",
    "    df.dropna(axis=1)\n",
    "\n",
    "Out\\[18\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # numpy & pandas is used for statistical ananlysis & for performing complex matematical operations \n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    df.dropna(axis=1,inplace=True)\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[21\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    df.drop(2)\n",
    "\n",
    "Out\\[22\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[24\\]:\n",
    "\n",
    "    df.drop([2,3])\n",
    "\n",
    "Out\\[24\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[29\\]:\n",
    "\n",
    "    # missing values are in the form of null values \n",
    "\n",
    "    df.drop([\"math\"],axis=1)\n",
    "\n",
    "Out\\[29\\]:\n",
    "\n",
    "|     | name   |\n",
    "|-----|--------|\n",
    "| 0   | john   |\n",
    "| 1   | jack   |\n",
    "| 2   | jhill  |\n",
    "| 3   | kenny  |\n",
    "| 4   | kelvin |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # when we are having the null values eiter we can drop the null values or replace the null values with the other values \n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[30\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    # to detect the null values if present in a dataset or not?\n",
    "\n",
    "    df.fillna(0)\n",
    "\n",
    "Out\\[31\\]:\n",
    "\n",
    "|     | name   | math                        |\n",
    "|-----|--------|-----------------------------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> |\n",
    "| 2   | jhill  | \\<module 'math' (built-in)> |\n",
    "| 3   | kenny  | \\<module 'math' (built-in)> |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> |\n",
    "\n",
    "In \\[32\\]:\n",
    "\n",
    "    df.isnull()\n",
    "\n",
    "    # isnull is used to check if we are having the null values \n",
    "\n",
    "Out\\[32\\]:\n",
    "\n",
    "|     | name  | math  |\n",
    "|-----|-------|-------|\n",
    "| 0   | False | False |\n",
    "| 1   | False | False |\n",
    "| 2   | False | False |\n",
    "| 3   | False | False |\n",
    "| 4   | False | False |\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    # to find the sum of the null values present in the dataset \n",
    "    # if the dataset is large then use below method \n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "Out\\[34\\]:\n",
    "\n",
    "    name    0\n",
    "    math    0\n",
    "    dtype: int64\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    # to drop null values\n",
    "\n",
    "    df.dropna\n",
    "\n",
    "Out\\[48\\]:\n",
    "\n",
    "    <bound method DataFrame.dropna of      name                        math  science\n",
    "    0    john  <module 'math' (built-in)>     78.0\n",
    "    1    jack  <module 'math' (built-in)>     58.0\n",
    "    2   jhill  <module 'math' (built-in)>      NaN\n",
    "    3   kenny  <module 'math' (built-in)>      NaN\n",
    "    4  kelvin  <module 'math' (built-in)>     79.0>\n",
    "\n",
    "In \\[49\\]:\n",
    "\n",
    "    df.dropna()\n",
    "\n",
    "Out\\[49\\]:\n",
    "\n",
    "|     | name   | math                        | science |\n",
    "|-----|--------|-----------------------------|---------|\n",
    "| 0   | john   | \\<module 'math' (built-in)> | 78.0    |\n",
    "| 1   | jack   | \\<module 'math' (built-in)> | 58.0    |\n",
    "| 4   | kelvin | \\<module 'math' (built-in)> | 79.0    |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # working on car csv file \n",
    "\n",
    "In \\[64\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "In \\[65\\]:\n",
    "\n",
    "    df=pd.read_csv(\"cars_data.csv\")\n",
    "\n",
    "In \\[66\\]:\n",
    "\n",
    "    df.head()\n",
    "\n",
    "Out\\[66\\]:\n",
    "\n",
    "|     | 3   | ?   | alfa-romero | gas | std | two  | convertible | rwd | front | 88.60 | ... | 130 | mpfi | 3.47 | 2.68 | 9.00 | 111 | 5000 | 21  | 27  | 13495 |\n",
    "|-----|-----|-----|-------------|-----|-----|------|-------------|-----|-------|-------|-----|-----|------|------|------|------|-----|------|-----|-----|-------|\n",
    "| 0   | 3   | ?   | alfa-romero | gas | std | two  | convertible | rwd | front | 88.6  | ... | 130 | mpfi | 3.47 | 2.68 | 9.0  | 111 | 5000 | 21  | 27  | 16500 |\n",
    "| 1   | 1   | ?   | alfa-romero | gas | std | two  | hatchback   | rwd | front | 94.5  | ... | 152 | mpfi | 2.68 | 3.47 | 9.0  | 154 | 5000 | 19  | 26  | 16500 |\n",
    "| 2   | 2   | 164 | audi        | gas | std | four | sedan       | fwd | front | 99.8  | ... | 109 | mpfi | 3.19 | 3.40 | 10.0 | 102 | 5500 | 24  | 30  | 13950 |\n",
    "| 3   | 2   | 164 | audi        | gas | std | four | sedan       | 4wd | front | 99.4  | ... | 136 | mpfi | 3.19 | 3.40 | 8.0  | 115 | 5500 | 18  | 22  | 17450 |\n",
    "| 4   | 2   | ?   | audi        | gas | std | two  | sedan       | fwd | front | 99.8  | ... | 136 | mpfi | 3.19 | 3.40 | 8.5  | 110 | 5500 | 19  | 25  | 15250 |\n",
    "\n",
    "5 rows × 26 columns\n",
    "\n",
    "In \\[67\\]:\n",
    "\n",
    "    df.head(3)\n",
    "\n",
    "Out\\[67\\]:\n",
    "\n",
    "|     | 3   | ?   | alfa-romero | gas | std | two  | convertible | rwd | front | 88.60 | ... | 130 | mpfi | 3.47 | 2.68 | 9.00 | 111 | 5000 | 21  | 27  | 13495 |\n",
    "|-----|-----|-----|-------------|-----|-----|------|-------------|-----|-------|-------|-----|-----|------|------|------|------|-----|------|-----|-----|-------|\n",
    "| 0   | 3   | ?   | alfa-romero | gas | std | two  | convertible | rwd | front | 88.6  | ... | 130 | mpfi | 3.47 | 2.68 | 9.0  | 111 | 5000 | 21  | 27  | 16500 |\n",
    "| 1   | 1   | ?   | alfa-romero | gas | std | two  | hatchback   | rwd | front | 94.5  | ... | 152 | mpfi | 2.68 | 3.47 | 9.0  | 154 | 5000 | 19  | 26  | 16500 |\n",
    "| 2   | 2   | 164 | audi        | gas | std | four | sedan       | fwd | front | 99.8  | ... | 109 | mpfi | 3.19 | 3.40 | 10.0 | 102 | 5500 | 24  | 30  | 13950 |\n",
    "\n",
    "3 rows × 26 columns\n",
    "\n",
    "In \\[69\\]:\n",
    "\n",
    "    df.isnull()\n",
    "\n",
    "    # isnull function only detects NAN values that is the null values \n",
    "\n",
    "Out\\[69\\]:\n",
    "\n",
    "|     | 3     | ?     | alfa-romero | gas   | std   | two   | convertible | rwd   | front | 88.60 | ... | 130   | mpfi  | 3.47  | 2.68  | 9.00  | 111   | 5000  | 21    | 27    | 13495 |\n",
    "|-----|-------|-------|-------------|-------|-------|-------|-------------|-------|-------|-------|-----|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
    "| 0   | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 1   | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 2   | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 3   | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 4   | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| ... | ...   | ...   | ...         | ...   | ...   | ...   | ...         | ...   | ...   | ...   | ... | ...   | ...   | ...   | ...   | ...   | ...   | ...   | ...   | ...   | ...   |\n",
    "| 199 | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 200 | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 201 | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 202 | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "| 203 | False | False | False       | False | False | False | False       | False | False | False | ... | False | False | False | False | False | False | False | False | False | False |\n",
    "\n",
    "204 rows × 26 columns\n",
    "\n",
    "In \\[70\\]:\n",
    "\n",
    "    df.isnull().sum()\n",
    "\n",
    "    # isnull.sum - it will give the sum of all the null values \n",
    "\n",
    "Out\\[70\\]:\n",
    "\n",
    "    3              0\n",
    "    ?              0\n",
    "    alfa-romero    0\n",
    "    gas            0\n",
    "    std            0\n",
    "    two            0\n",
    "    convertible    0\n",
    "    rwd            0\n",
    "    front          0\n",
    "    88.60          0\n",
    "    168.80         0\n",
    "    64.10          0\n",
    "    48.80          0\n",
    "    2548           0\n",
    "    dohc           0\n",
    "    four           0\n",
    "    130            0\n",
    "    mpfi           0\n",
    "    3.47           0\n",
    "    2.68           0\n",
    "    9.00           0\n",
    "    111            0\n",
    "    5000           0\n",
    "    21             0\n",
    "    27             0\n",
    "    13495          0\n",
    "    dtype: int64\n",
    "\n",
    "In \\[71\\]:\n",
    "\n",
    "    df.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 204 entries, 0 to 203\n",
    "    Data columns (total 26 columns):\n",
    "     #   Column       Non-Null Count  Dtype  \n",
    "    ---  ------       --------------  -----  \n",
    "     0   3            204 non-null    int64  \n",
    "     1   ?            204 non-null    object \n",
    "     2   alfa-romero  204 non-null    object \n",
    "     3   gas          204 non-null    object \n",
    "     4   std          204 non-null    object \n",
    "     5   two          204 non-null    object \n",
    "     6   convertible  204 non-null    object \n",
    "     7   rwd          204 non-null    object \n",
    "     8   front        204 non-null    object \n",
    "     9   88.60        204 non-null    float64\n",
    "     10  168.80       204 non-null    float64\n",
    "     11  64.10        204 non-null    float64\n",
    "     12  48.80        204 non-null    float64\n",
    "     13  2548         204 non-null    int64  \n",
    "     14  dohc         204 non-null    object \n",
    "     15  four         204 non-null    object \n",
    "     16  130          204 non-null    int64  \n",
    "     17  mpfi         204 non-null    object \n",
    "     18  3.47         204 non-null    object \n",
    "     19  2.68         204 non-null    object \n",
    "     20  9.00         204 non-null    float64\n",
    "     21  111          204 non-null    object \n",
    "     22  5000         204 non-null    object \n",
    "     23  21           204 non-null    int64  \n",
    "     24  27           204 non-null    int64  \n",
    "     25  13495        204 non-null    object \n",
    "    dtypes: float64(5), int64(5), object(16)\n",
    "    memory usage: 41.6+ KB\n",
    "\n",
    "In \\[72\\]:\n",
    "\n",
    "    df.describe()\n",
    "\n",
    "    # describe will give the numerical elements present in the dataset \n",
    "\n",
    "Out\\[72\\]:\n",
    "\n",
    "|       | 3          | 88.60      | 168.80     | 64.10      | 48.80      | 2548        | 130        | 9.00       | 21         | 27         |\n",
    "|-------|------------|------------|------------|------------|------------|-------------|------------|------------|------------|------------|\n",
    "| count | 204.000000 | 204.000000 | 204.000000 | 204.000000 | 204.000000 | 204.000000  | 204.000000 | 204.000000 | 204.000000 | 204.000000 |\n",
    "| mean  | 0.823529   | 98.806373  | 174.075000 | 65.916667  | 53.749020  | 2555.602941 | 126.892157 | 10.148137  | 25.240196  | 30.769608  |\n",
    "| std   | 1.239035   | 5.994144   | 12.362123  | 2.146716   | 2.424901   | 521.960820  | 41.744569  | 3.981000   | 6.551513   | 6.898337   |\n",
    "| min   | -2.000000  | 86.600000  | 141.100000 | 60.300000  | 47.800000  | 1488.000000 | 61.000000  | 7.000000   | 13.000000  | 16.000000  |\n",
    "| 25%   | 0.000000   | 94.500000  | 166.300000 | 64.075000  | 52.000000  | 2145.000000 | 97.000000  | 8.575000   | 19.000000  | 25.000000  |\n",
    "| 50%   | 1.000000   | 97.000000  | 173.200000 | 65.500000  | 54.100000  | 2414.000000 | 119.500000 | 9.000000   | 24.000000  | 30.000000  |\n",
    "| 75%   | 2.000000   | 102.400000 | 183.200000 | 66.900000  | 55.500000  | 2939.250000 | 142.000000 | 9.400000   | 30.000000  | 34.500000  |\n",
    "| max   | 3.000000   | 120.900000 | 208.100000 | 72.300000  | 59.800000  | 4066.000000 | 326.000000 | 23.000000  | 49.000000  | 54.000000  |\n",
    "\n",
    "In \\[73\\]:\n",
    "\n",
    "    df[\"convertible\"].unique()\n",
    "\n",
    "    # unique will give the unique values of the particular column \n",
    "\n",
    "Out\\[73\\]:\n",
    "\n",
    "    array(['convertible', 'hatchback', 'sedan', 'wagon', 'hardtop'],\n",
    "          dtype=object)\n",
    "\n",
    "In \\[74\\]:\n",
    "\n",
    "    df[\"convertible\"].value_counts()\n",
    "\n",
    "    # it will give the value counts of the particular column \n",
    "\n",
    "Out\\[74\\]:\n",
    "\n",
    "    convertible\n",
    "    sedan          96\n",
    "    hatchback      70\n",
    "    wagon          25\n",
    "    hardtop         8\n",
    "    convertible     5\n",
    "    Name: count, dtype: int64\n",
    "\n",
    "In \\[75\\]:\n",
    "\n",
    "    df[\"convertible\"].replace(\"?\",np.nan,inplace=True)\n",
    "\n",
    "    # here we will replace the ? mark with nan \n",
    "\n",
    "In \\[76\\]:\n",
    "\n",
    "    df\n",
    "\n",
    "Out\\[76\\]:\n",
    "\n",
    "|     | 3   | ?   | alfa-romero | gas    | std   | two  | convertible | rwd | front | 88.60 | ... | 130 | mpfi | 3.47 | 2.68 | 9.00 | 111 | 5000 | 21  | 27  | 13495 |\n",
    "|-----|-----|-----|-------------|--------|-------|------|-------------|-----|-------|-------|-----|-----|------|------|------|------|-----|------|-----|-----|-------|\n",
    "| 0   | 3   | ?   | alfa-romero | gas    | std   | two  | convertible | rwd | front | 88.6  | ... | 130 | mpfi | 3.47 | 2.68 | 9.0  | 111 | 5000 | 21  | 27  | 16500 |\n",
    "| 1   | 1   | ?   | alfa-romero | gas    | std   | two  | hatchback   | rwd | front | 94.5  | ... | 152 | mpfi | 2.68 | 3.47 | 9.0  | 154 | 5000 | 19  | 26  | 16500 |\n",
    "| 2   | 2   | 164 | audi        | gas    | std   | four | sedan       | fwd | front | 99.8  | ... | 109 | mpfi | 3.19 | 3.40 | 10.0 | 102 | 5500 | 24  | 30  | 13950 |\n",
    "| 3   | 2   | 164 | audi        | gas    | std   | four | sedan       | 4wd | front | 99.4  | ... | 136 | mpfi | 3.19 | 3.40 | 8.0  | 115 | 5500 | 18  | 22  | 17450 |\n",
    "| 4   | 2   | ?   | audi        | gas    | std   | two  | sedan       | fwd | front | 99.8  | ... | 136 | mpfi | 3.19 | 3.40 | 8.5  | 110 | 5500 | 19  | 25  | 15250 |\n",
    "| ... | ... | ... | ...         | ...    | ...   | ...  | ...         | ... | ...   | ...   | ... | ... | ...  | ...  | ...  | ...  | ... | ...  | ... | ... | ...   |\n",
    "| 199 | -1  | 95  | volvo       | gas    | std   | four | sedan       | rwd | front | 109.1 | ... | 141 | mpfi | 3.78 | 3.15 | 9.5  | 114 | 5400 | 23  | 28  | 16845 |\n",
    "| 200 | -1  | 95  | volvo       | gas    | turbo | four | sedan       | rwd | front | 109.1 | ... | 141 | mpfi | 3.78 | 3.15 | 8.7  | 160 | 5300 | 19  | 25  | 19045 |\n",
    "| 201 | -1  | 95  | volvo       | gas    | std   | four | sedan       | rwd | front | 109.1 | ... | 173 | mpfi | 3.58 | 2.87 | 8.8  | 134 | 5500 | 18  | 23  | 21485 |\n",
    "| 202 | -1  | 95  | volvo       | diesel | turbo | four | sedan       | rwd | front | 109.1 | ... | 145 | idi  | 3.01 | 3.40 | 23.0 | 106 | 4800 | 26  | 27  | 22470 |\n",
    "| 203 | -1  | 95  | volvo       | gas    | turbo | four | sedan       | rwd | front | 109.1 | ... | 141 | mpfi | 3.78 | 3.15 | 9.5  | 114 | 5400 | 19  | 25  | 22625 |\n",
    "\n",
    "204 rows × 26 columns\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # handling outliers \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # for handling boxplot the box plot is used \n",
    "    # with the help of IQR we can measure the outliers \n",
    "    # make sure that you plot the box plot on 2 variables \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # how to remove outliers ?\n",
    "    # Identify Outliers: Use statistical methods such as z-scores, interquartile range (IQR), or visualization techniques like box plots to identify data points that fall significantly outside the rest of the data.\n",
    "\n",
    "    Choose a Threshold: Determine a threshold beyond which data points are considered outliers. This could be a certain number of standard deviations from the mean, a range of values outside the quartiles, or a domain-specific criterion.\n",
    "\n",
    "    Decide on Removal Strategy: Decide whether to remove outliers entirely, replace them with a more typical value (e.g., the median), or leave them unchanged depending on the nature of your data and analysis goals.\n",
    "\n",
    "    Remove or Transform Outliers: Implement your chosen strategy to handle outliers. This might involve deleting them from the dataset, replacing them with a more representative value, or transforming them in some way (e.g., winsorizing).\n",
    "\n",
    "    Validate Results: After removing outliers, validate the impact on your analysis or model performance. Ensure that removing outliers doesn't introduce bias or significantly alter the conclusions drawn from the data.\n",
    "\n",
    "    Document: Document the outlier removal process thoroughly, including the criteria used for identifying outliers and the rationale behind the chosen removal strategy. This helps maintain transparency and reproducibility in your analysis.\n",
    "        \n",
    "    # what are the techniques to remove outliers?\n",
    "\n",
    "    There are several techniques to remove outliers from a dataset:\n",
    "\n",
    "    1. **Standard Deviation Method**: Identify data points that fall more than a certain number of standard deviations away from the mean and consider them outliers. This method assumes a normal distribution of data.\n",
    "\n",
    "    2. **Interquartile Range (IQR) Method**: Calculate the IQR, which is the difference between the third quartile (Q3) and the first quartile (Q1), and define outliers as data points outside the range of \\( Q1 - 1.5 \\times \\text{IQR} \\) to \\( Q3 + 1.5 \\times \\text{IQR} \\).\n",
    "\n",
    "    3. **Z-Score Method**: Compute the z-score for each data point, which represents the number of standard deviations it is away from the mean. Data points with z-scores beyond a certain threshold (e.g., 3 or -3) are considered outliers.\n",
    "\n",
    "    4. **Box Plot Method**: Visualize the data using a box plot and identify outliers as data points that fall outside the \"whiskers\" of the plot, typically defined as \\( Q1 - 1.5 \\times \\text{IQR} \\) and \\( Q3 + 1.5 \\times \\text{IQR} \\).\n",
    "\n",
    "    5. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Use a clustering algorithm like DBSCAN to identify clusters of data points and classify points that are not part of any cluster as outliers.\n",
    "\n",
    "    6. **Isolation Forest**: Apply an isolation forest algorithm, which identifies outliers by isolating data points in a tree structure and measuring the number of splits required to isolate each point.\n",
    "\n",
    "    7. **Manual Inspection**: Sometimes, outliers are best identified through domain knowledge and manual inspection of the data. This approach is particularly useful when dealing with complex or non-standard data distributions.\n",
    "\n",
    "    8. **Winsorization**: Instead of removing outliers, winsorization involves replacing extreme values with less extreme values, typically by setting them to a specified percentile of the data distribution (e.g., the 5th and 95th percentiles).\n",
    "\n",
    "    The choice of technique depends on factors such as the distribution of the data, the nature of outliers, and the specific goals of the analysis. It's often beneficial to explore multiple techniques and compare their effects on the dataset before making a final decision.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # skewness \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # it is measure of asymeetric \n",
    "    # if the data is not normally distributed the data is having skewness  \n",
    "    # positive skewness - the tail is on the positive side \n",
    "    # negative skewness - the tail is on the negative side \n",
    "    # how to deal with skewness \n",
    "    # either we can do square root or log transformation with the real data \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # how to handle categorical data ?\n",
    "\n",
    "    # Handling categorical data involves several techniques depending on the nature of the data and the analysis being performed. Here are some common approaches:\n",
    "\n",
    "    1. **Label Encoding**: Convert categorical variables into numerical labels. Each category is assigned a unique integer. This method is useful when the categorical variable has an inherent ordinal relationship, but it may not be suitable for variables without such a relationship, as it can introduce unintended ordinality.\n",
    "\n",
    "    2. **One-Hot Encoding**: Create binary columns for each category in the categorical variable. Each category is represented by a binary feature (0 or 1). This method preserves the information without assuming any ordinal relationship between categories. However, it can lead to a high-dimensional dataset, especially with a large number of categories.\n",
    "\n",
    "    3. **Dummy Coding**: Similar to one-hot encoding, but it omits one category as a reference category. This reduces the dimensionality by one but still preserves the information about the categories.\n",
    "\n",
    "    4. **Ordinal Encoding**: Assign numerical values to categories based on their order or some other meaningful relationship. This method is suitable for categorical variables with a clear ordinal relationship, such as \"low,\" \"medium,\" and \"high.\" However, it requires careful consideration of the underlying meaning of the categories.\n",
    "\n",
    "    5. **Target Encoding**: Replace each category with the mean (or another statistical measure) of the target variable for that category. This method can be useful for categorical variables with high cardinality but may lead to overfitting if not used carefully.\n",
    "\n",
    "    6. **Frequency Encoding**: Replace each category with the frequency of occurrence in the dataset. This method can capture the importance of each category based on its prevalence in the data.\n",
    "\n",
    "    7. **Embedding**: For deep learning models, categorical variables can be embedded into lower-dimensional continuous vectors, allowing the model to learn representations of the categories during training.\n",
    "\n",
    "    8. **Hash Encoding**: Hash the categories into a fixed number of buckets, reducing the dimensionality of the encoded features. While simple and efficient, it may lead to collisions where different categories are mapped to the same bucket.\n",
    "\n",
    "    The choice of encoding method depends on factors such as the nature of the categorical variable, the size of the dataset, and the specific requirements of the analysis or model. It's important to consider the implications of each method, such as potential information loss, dimensionality, and computational complexity. Additionally, it's essential to handle missing values appropriately before encoding categorical data.\n",
    "\n",
    "    # the simplest way  to convert is categorical data into the numerical data .\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # what is one hot encoding ?\n",
    "    # One-hot encoding is a technique used in machine learning and data processing to represent categorical data numerically. It's particularly useful when dealing with categorical variables in a dataset that need to be included in machine learning algorithms. \n",
    "\n",
    "    Here's how it works: \n",
    "    - Let's say you have a categorical variable with `n` distinct categories.\n",
    "    - One-hot encoding represents each category as a binary vector of length `n`.\n",
    "    - Each binary vector has all zeros except for one element, which corresponds to the category it represents. \n",
    "    - If a data point belongs to a particular category, the corresponding element in its one-hot encoded vector is set to 1, and all other elements are set to 0.\n",
    "\n",
    "    For example, consider a categorical variable \"Color\" with three categories: Red, Green, and Blue. After one-hot encoding, each category would be represented as follows:\n",
    "    - Red: [1, 0, 0]\n",
    "    - Green: [0, 1, 0]\n",
    "    - Blue: [0, 0, 1]\n",
    "\n",
    "    This representation allows machine learning algorithms to handle categorical data more effectively, as it ensures that the numerical values assigned to different categories don't imply any ordinal relationship between them.\n",
    "\n",
    "    # what is label encoding?\n",
    "    # Label encoding is another technique used in machine learning and data preprocessing to transform categorical data into numerical data. Unlike one-hot encoding, which represents each category with a binary vector, label encoding assigns a unique numerical label to each category.\n",
    "\n",
    "    Here's how label encoding works:\n",
    "    - Each unique category in the categorical variable is assigned a numerical label, typically starting from 0 or 1 and incrementing by one for each subsequent category.\n",
    "    - The categorical variable is then replaced with the corresponding numerical labels.\n",
    "\n",
    "    For example, consider the categorical variable \"City\" with three categories: New York, London, and Paris. After label encoding, the categories might be represented as follows:\n",
    "    - New York: 0\n",
    "    - London: 1\n",
    "    - Paris: 2\n",
    "\n",
    "    Label encoding is useful when the categorical variable has an inherent ordinal relationship, meaning that the categories can be ordered or ranked. However, it's essential to be cautious when using label encoding with categorical variables that do not have such a natural order, as it may introduce unintended relationships or biases in the data. In such cases, one-hot encoding is often preferred.\n",
    "\n",
    "    # difference between one hot encoding & label encoding?\n",
    "    # The main differences between one-hot encoding and label encoding lie in how they represent categorical data numerically and their respective use cases:\n",
    "\n",
    "    1. Representation:\n",
    "       - One-hot encoding represents each category with a binary vector, where only one element is set to 1 (indicating the presence of that category) and all other elements are set to 0.\n",
    "       - Label encoding assigns a unique numerical label to each category, typically starting from 0 or 1 and incrementing by one for each subsequent category.\n",
    "\n",
    "    2. Dimensionality:\n",
    "       - One-hot encoding increases the dimensionality of the dataset, with each category being represented by a separate binary feature.\n",
    "       - Label encoding does not increase the dimensionality; it simply replaces each category with a numerical label.\n",
    "\n",
    "    3. Relationship:\n",
    "       - One-hot encoding does not impose any ordinal relationship between categories. Each category is represented independently of others.\n",
    "       - Label encoding assumes an ordinal relationship between categories. The numerical labels imply an order or rank among the categories.\n",
    "\n",
    "    4. Use Cases:\n",
    "       - One-hot encoding is suitable for categorical variables where there is no inherent order or ranking among categories. It is commonly used when dealing with nominal categorical variables.\n",
    "       - Label encoding is useful when the categorical variable has an inherent ordinal relationship, meaning that the categories can be ordered or ranked. It is often used with ordinal categorical variables.\n",
    "\n",
    "    In summary, while both techniques are used to encode categorical data numerically, they differ in their representation, dimensionality, assumptions about the relationships between categories, and suitable use cases.\n",
    "\n",
    "    # what is get_dummies?\n",
    "    # `get_dummies()` is a function provided by the pandas library in Python, primarily used for one-hot encoding categorical variables in a DataFrame. It transforms categorical variables into dummy/indicator variables, creating binary columns for each category in the original variable.\n",
    "\n",
    "    Here's how `get_dummies()` works:\n",
    "    1. It takes a DataFrame or Series containing categorical variables as input.\n",
    "    2. For each categorical variable, it creates new binary columns corresponding to each unique category.\n",
    "    3. The values in these binary columns are set to 1 if the observation belongs to that category and 0 otherwise.\n",
    "\n",
    "    Here's an example:\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with a categorical variable\n",
    "    data = {'City': ['New York', 'London', 'Paris', 'London', 'New York']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Use get_dummies() to one-hot encode the 'City' variable\n",
    "    one_hot_encoded = pd.get_dummies(df['City'])\n",
    "\n",
    "    print(one_hot_encoded)\n",
    "    ```\n",
    "\n",
    "    Output:\n",
    "    ```\n",
    "       London  New York  Paris\n",
    "    0       0         1      0\n",
    "    1       1         0      0\n",
    "    2       0         0      1\n",
    "    3       1         0      0\n",
    "    4       0         1      0\n",
    "    ```\n",
    "\n",
    "    In this example, the `City` variable is one-hot encoded using `get_dummies()`. Three new binary columns are created (`London`, `New York`, `Paris`), each representing one of the unique categories in the original `City` variable.\n",
    "\n",
    "    `get_dummies()` is a convenient and efficient way to perform one-hot encoding in pandas, especially when dealing with categorical variables in machine learning datasets.\n",
    "\n",
    "    # what is normalization?\n",
    "    # Normalization is a preprocessing technique used in machine learning and data analysis to scale numeric features to a standard range. The goal of normalization is to bring all features to a similar scale without distorting differences in the ranges of values. This ensures that no particular feature dominates solely because of its larger magnitude.\n",
    "\n",
    "    There are several methods of normalization, but one common approach is Min-Max scaling, which scales the values of a feature to a fixed range, typically between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "    \\[ X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "    Where:\n",
    "    - \\(X\\) is an original value of a feature.\n",
    "    - \\(X_{\\text{min}}\\) is the minimum value of that feature in the dataset.\n",
    "    - \\(X_{\\text{max}}\\) is the maximum value of that feature in the dataset.\n",
    "\n",
    "    Another common normalization technique is Z-score normalization (Standardization), which scales the values to have a mean of 0 and a standard deviation of 1. The formula for Z-score normalization is:\n",
    "\n",
    "    \\[ X_{\\text{normalized}} = \\frac{X - \\mu}{\\sigma} \\]\n",
    "\n",
    "    Where:\n",
    "    - \\(X\\) is an original value of a feature.\n",
    "    - \\(\\mu\\) is the mean of that feature in the dataset.\n",
    "    - \\(\\sigma\\) is the standard deviation of that feature in the dataset.\n",
    "\n",
    "    Normalization helps in improving the performance of machine learning algorithms by ensuring that features with larger scales do not disproportionately influence the learning process. It is particularly useful for algorithms that rely on distance measures, such as k-nearest neighbors and clustering algorithms, where features with larger scales can dominate the distance calculation.\n",
    "\n",
    "    # what is scaler , minmax scaler?\n",
    "    # A scaler, in the context of machine learning and data preprocessing, is a method used to scale numerical features to a predefined range or distribution. It ensures that all features have similar scales, which can be beneficial for certain machine learning algorithms that are sensitive to the scale of features.\n",
    "\n",
    "    MinMaxScaler is a specific type of scaler used for normalization, specifically Min-Max scaling. It scales the features to a fixed range, usually between 0 and 1. The MinMaxScaler in scikit-learn provides an easy way to perform this scaling.\n",
    "\n",
    "    Here's how MinMaxScaler works in scikit-learn:\n",
    "    1. Initialize the MinMaxScaler object.\n",
    "    2. Fit the scaler to your data using the `fit()` method. This calculates the minimum and maximum values of each feature.\n",
    "    3. Transform your data using the `transform()` method. This applies the scaling transformation to your data based on the minimum and maximum values calculated during the fitting step.\n",
    "\n",
    "    Here's an example of how to use MinMaxScaler in scikit-learn:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import numpy as np\n",
    "\n",
    "    # Sample data\n",
    "    data = np.array([[1.0, 2.0, 3.0],\n",
    "                     [4.0, 5.0, 6.0],\n",
    "                     [7.0, 8.0, 9.0]])\n",
    "\n",
    "    # Initialize MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit scaler to data\n",
    "    scaler.fit(data)\n",
    "\n",
    "    # Transform data\n",
    "    scaled_data = scaler.transform(data)\n",
    "\n",
    "    print(\"Scaled data:\")\n",
    "    print(scaled_data)\n",
    "    ```\n",
    "\n",
    "    Output:\n",
    "    ```\n",
    "    Scaled data:\n",
    "    [[0.  0.  0. ]\n",
    "     [0.5 0.5 0.5]\n",
    "     [1.  1.  1. ]]\n",
    "    ```\n",
    "\n",
    "    In this example, MinMaxScaler scales each feature to a range between 0 and 1, based on the minimum and maximum values of each feature in the original data.\n",
    "\n",
    "    # what is linear regression & types of regression?\n",
    "    # Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The goal of linear regression is to find the best-fitting line (or hyperplane in the case of multiple independent variables) that minimizes the sum of the squared differences between the observed and predicted values.\n",
    "\n",
    "    The linear regression model is represented by the equation:\n",
    "\n",
    "    \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon \\]\n",
    "\n",
    "    Where:\n",
    "    - \\( y \\) is the dependent variable (the variable being predicted).\n",
    "    - \\( x_1, x_2, \\ldots, x_n \\) are the independent variables (features).\n",
    "    - \\( \\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n \\) are the coefficients (parameters) of the model.\n",
    "    - \\( \\varepsilon \\) represents the error term, which accounts for the discrepancy between the observed and predicted values.\n",
    "\n",
    "    Types of regression include:\n",
    "\n",
    "    1. **Simple Linear Regression**: It involves only one independent variable.\n",
    "       \\[ y = \\beta_0 + \\beta_1 x + \\varepsilon \\]\n",
    "\n",
    "    2. **Multiple Linear Regression**: It involves two or more independent variables.\n",
    "       \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon \\]\n",
    "\n",
    "    3. **Polynomial Regression**: It uses polynomial functions to model the relationship between the independent and dependent variables.\n",
    "       \\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_n x^n + \\varepsilon \\]\n",
    "\n",
    "    4. **Ridge Regression**: It is a regularized version of linear regression that includes a penalty term to the coefficients to prevent overfitting.\n",
    "       \\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "\n",
    "    5. **Lasso Regression**: Similar to ridge regression, it is a regularized version of linear regression that uses the L1 norm penalty term to induce sparsity in the coefficient estimates.\n",
    "       \\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "\n",
    "    6. **ElasticNet Regression**: It combines both L1 and L2 regularization penalties to overcome the limitations of ridge and lasso regression.\n",
    "       \\[ \\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "\n",
    "    7. **Logistic Regression**: It's used for binary classification problems and predicts the probability that a given observation belongs to a particular class.\n",
    "       \\[ P(y=1) = \\frac{1}{1 + e^{-z}} \\]\n",
    "\n",
    "    These are just a few examples, and there are many other variants and extensions of linear regression techniques used in different contexts and for various types of data.\n",
    "\n",
    "    # what is homoscedecity ?\n",
    "    # Homoscedasticity, also known as homogeneity of variance, is a key assumption of linear regression and other statistical models. It refers to the property where the variance of the residuals (the differences between observed and predicted values) is constant across all levels of the independent variables. In simpler terms, it means that the spread of the residuals is consistent across the range of predicted values.\n",
    "\n",
    "    When a regression model exhibits homoscedasticity, the scatter plot of residuals against predicted values should show a random pattern with no discernible trend or cone-shaped pattern. This indicates that the variability of the residuals remains constant as the predicted values change.\n",
    "\n",
    "    Homoscedasticity is essential because violating this assumption can lead to biased and inefficient estimates of the regression coefficients and incorrect inferences about the relationships between variables. It can also affect the accuracy and reliability of predictions made by the model.\n",
    "\n",
    "    If homoscedasticity is not met, it may be necessary to transform the dependent variable or the independent variables, or consider using alternative modeling techniques that are robust to heteroscedasticity, such as robust regression methods. Additionally, techniques like weighted least squares regression can be employed to address heteroscedasticity by assigning different weights to observations based on their variance.\n",
    "\n",
    "    # diffrentiate between overfitting & uderfitting?\n",
    "    # Overfitting and underfitting are two common problems encountered when building machine learning models, particularly with techniques like regression and classification. They relate to how well the model generalizes to unseen data.\n",
    "\n",
    "    1. **Overfitting**:\n",
    "       - Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data rather than the underlying pattern.\n",
    "       - Signs of overfitting include:\n",
    "         - The model performs well on the training data but poorly on unseen data (test/validation data).\n",
    "         - The model has high variance, meaning it is too sensitive to the training data and fails to generalize to new data.\n",
    "         - The model may have overly complex decision boundaries or exhibit erratic behavior.\n",
    "       - Overfitting often happens when the model is too complex relative to the amount and quality of the training data, or when the training data is noisy.\n",
    "\n",
    "    2. **Underfitting**:\n",
    "       - Underfitting occurs when a model is too simplistic to capture the underlying structure of the data, resulting in poor performance on both the training and test/validation data.\n",
    "       - Signs of underfitting include:\n",
    "         - The model has high bias, meaning it is too simple to capture the relationships in the data.\n",
    "         - The model performs poorly on both the training and test/validation data.\n",
    "         - The model may exhibit a systematic error, consistently underpredicting or overpredicting the target variable.\n",
    "       - Underfitting often happens when the model is too simple or when important features or relationships in the data are not captured.\n",
    "\n",
    "    In summary, overfitting occurs when a model is too complex and learns the noise in the training data, while underfitting occurs when a model is too simplistic and fails to capture the underlying patterns in the data. The goal is to find the right balance between complexity and generalization by selecting an appropriate model complexity and using techniques like cross-validation and regularization to mitigate overfitting and underfitting.\n",
    "\n",
    "    # what is bias - variance trade off?\n",
    "    # The bias-variance tradeoff is a fundamental concept in machine learning that deals with the relationship between the bias of a model and its variance. It describes the balance between the model's ability to capture the true underlying pattern in the data (bias) and its sensitivity to variations in the training data (variance).\n",
    "\n",
    "    1. **Bias**:\n",
    "       - Bias refers to the error introduced by approximating a real-world problem with a simplified model. It represents the difference between the average prediction of our model and the true value we are trying to predict. A high bias model tends to oversimplify the problem and may underfit the data.\n",
    "\n",
    "    2. **Variance**:\n",
    "       - Variance refers to the variability of model predictions for a given data point. It measures the model's sensitivity to fluctuations in the training data. A high variance model is overly sensitive to noise in the training data and may overfit by capturing random fluctuations instead of the underlying pattern.\n",
    "\n",
    "    The tradeoff arises because decreasing bias often leads to an increase in variance, and vice versa. Here's how it works:\n",
    "\n",
    "    - **High Bias, Low Variance**:\n",
    "      - Models with high bias and low variance tend to be overly simplistic and may underfit the data. They make strong assumptions about the underlying pattern, which may not capture the true complexities of the data.\n",
    "\n",
    "    - **Low Bias, High Variance**:\n",
    "      - Models with low bias and high variance are more flexible and may capture the underlying pattern well, but they are also more sensitive to noise and fluctuations in the training data. They may overfit by fitting the training data too closely and failing to generalize to new data.\n",
    "\n",
    "    The goal in machine learning is to find the optimal balance between bias and variance. This typically involves selecting a model complexity that minimizes the total error on unseen data, known as the expected prediction error. Techniques such as cross-validation, regularization, and ensemble methods are commonly used to manage the bias-variance tradeoff and improve the performance of machine learning models.\n",
    "\n",
    "    # what is cross validation & types of cross validation ?\n",
    "    # Cross-validation is a resampling technique used in machine learning to assess how well a predictive model generalizes to an independent dataset. It involves partitioning the available data into multiple subsets, known as folds, and iteratively training and evaluating the model on different combinations of these folds.\n",
    "\n",
    "    The primary purpose of cross-validation is to estimate the model's performance on unseen data and to detect and prevent issues such as overfitting. By averaging the performance metrics across multiple folds, cross-validation provides a more reliable estimate of the model's performance than simply using a single train-test split.\n",
    "\n",
    "    Here's how cross-validation works:\n",
    "\n",
    "    1. **Partitioning the Data**:\n",
    "       - The dataset is divided into \\( k \\) approximately equal-sized folds. The most common choice for \\( k \\) is 5 or 10, but other values can also be used.\n",
    "       - Each fold contains a roughly equal proportion of the original dataset's samples.\n",
    "\n",
    "    2. **Training and Evaluation**:\n",
    "       - For each iteration, one fold is held out as the validation set, and the remaining folds are used as the training set.\n",
    "       - The model is trained on the training set and evaluated on the validation set using a chosen performance metric (e.g., accuracy, mean squared error).\n",
    "       - This process is repeated \\( k \\) times, with each fold serving as the validation set exactly once.\n",
    "\n",
    "    3. **Aggregating Results**:\n",
    "       - The performance metrics obtained from each iteration are averaged to obtain a single estimate of the model's performance.\n",
    "\n",
    "    Types of cross-validation techniques include:\n",
    "\n",
    "    1. **K-Fold Cross-Validation**:\n",
    "       - The dataset is divided into \\( k \\) equal-sized folds.\n",
    "       - In each iteration, one fold is held out as the validation set, and the remaining \\( k - 1 \\) folds are used as the training set.\n",
    "\n",
    "    2. **Stratified K-Fold Cross-Validation**:\n",
    "       - Similar to k-fold cross-validation, but it ensures that each fold has approximately the same proportion of samples from each class as the original dataset. This is particularly useful for classification tasks with imbalanced class distributions.\n",
    "\n",
    "    3. **Leave-One-Out Cross-Validation (LOOCV)**:\n",
    "       - Each fold consists of a single sample from the dataset.\n",
    "       - The model is trained on all samples except one and evaluated on the left-out sample.\n",
    "       - This process is repeated for each sample in the dataset.\n",
    "\n",
    "    4. **Repeated K-Fold Cross-Validation**:\n",
    "       - K-fold cross-validation is repeated multiple times with different random partitions of the data.\n",
    "       - This helps to obtain a more robust estimate of the model's performance by reducing the variability associated with a single partitioning of the data.\n",
    "\n",
    "    5. **Time Series Cross-Validation**:\n",
    "       - Specifically designed for time series data, where the order of observations matters.\n",
    "       - It involves splitting the data into consecutive blocks or windows of time, ensuring that the validation sets follow the training sets chronologically.\n",
    "\n",
    "    Each type of cross-validation has its advantages and disadvantages, and the choice of technique depends on factors such as the size and nature of the dataset, the presence of class imbalance, and computational resources available.\n",
    "\n",
    "    # what is boosting?\n",
    "    # Boosting is a powerful ensemble learning technique used in machine learning to improve the predictive performance of a collection of weak learners (models) by combining them into a strong learner. The basic idea behind boosting is to iteratively train a sequence of weak learners, where each subsequent learner focuses on the mistakes made by the previous ones. By doing so, boosting aims to reduce bias and variance, leading to improved predictive accuracy.\n",
    "\n",
    "    Here's how boosting typically works:\n",
    "\n",
    "    1. **Initialize Model**:\n",
    "       - The process begins by training an initial weak learner, such as a decision tree, on the entire dataset.\n",
    "\n",
    "    2. **Iterative Training**:\n",
    "       - In subsequent iterations, the focus is on the instances in the dataset that were misclassified by the previous weak learners.\n",
    "       - Each new weak learner is trained on a modified version of the dataset, where the weights of misclassified instances are increased, and correctly classified instances are down-weighted or ignored.\n",
    "       - The goal is to prioritize the difficult instances that were not well handled by the previous weak learners.\n",
    "\n",
    "    3. **Combine Weak Learners**:\n",
    "       - The predictions of all weak learners are then combined through a weighted sum or voting scheme to form the final ensemble model.\n",
    "       - Typically, each weak learner's contribution to the final prediction is weighted based on its individual performance.\n",
    "\n",
    "    Boosting algorithms differ in how they update the weights of instances and how they combine weak learners. Some popular boosting algorithms include:\n",
    "\n",
    "    1. **AdaBoost (Adaptive Boosting)**:\n",
    "       - In AdaBoost, each weak learner is trained sequentially, and at each iteration, the weights of misclassified instances are increased. Subsequent weak learners focus more on the misclassified instances, gradually improving the model's performance.\n",
    "\n",
    "    2. **Gradient Boosting**:\n",
    "       - Gradient Boosting builds a sequence of weak learners in a forward stage-wise manner. Each new learner is trained to correct the residual errors of the previous ones.\n",
    "       - Instead of adjusting instance weights, gradient boosting fits each weak learner to the residual errors (or gradients) of the previous weak learners, gradually reducing the errors with each iteration.\n",
    "\n",
    "    3. **XGBoost (Extreme Gradient Boosting)**:\n",
    "       - XGBoost is an optimized implementation of gradient boosting, known for its scalability, speed, and performance.\n",
    "       - It introduces additional regularization terms to control model complexity and overfitting, making it highly effective for a wide range of datasets.\n",
    "\n",
    "    Boosting is a versatile technique that can be applied to various types of weak learners and is known for producing highly accurate predictive models, particularly in structured data and tabular data problems. However, boosting algorithms can be sensitive to noisy data and outliers, and they may require careful tuning of hyperparameters to achieve optimal performance.\n",
    "\n",
    "    # what is ada boost ?\n",
    "    # AdaBoost, short for Adaptive Boosting, is a popular ensemble learning technique used in machine learning for classification and regression tasks. It works by combining the predictions of multiple weak learners, typically decision trees, to create a strong learner. \n",
    "\n",
    "    Here's how it generally works:\n",
    "\n",
    "    1. **Initialize Sample Weights**: Initially, all data points are given equal weights.\n",
    "\n",
    "    2. **Train Weak Learner**: A weak learner (often a decision tree) is trained on the data. It could be a simple model that performs slightly better than random chance.\n",
    "\n",
    "    3. **Compute Error**: The error of the weak learner is computed by comparing its predictions with the actual labels, weighted by the sample weights.\n",
    "\n",
    "    4. **Update Weights**: Increase the weights of the misclassified data points so that the next weak learner focuses more on the examples that were previously misclassified. Decrease the weights of correctly classified points.\n",
    "\n",
    "    5. **Repeat**: Steps 2-4 are repeated iteratively. Each subsequent weak learner focuses more on the examples that were previously misclassified by the ensemble.\n",
    "\n",
    "    6. **Combine Learners**: The weak learners are combined into a strong learner by assigning weights to their predictions based on their performance.\n",
    "\n",
    "    7. **Final Prediction**: The final prediction is made by aggregating the predictions of all weak learners, usually through a weighted majority vote (for classification) or a weighted average (for regression).\n",
    "\n",
    "    AdaBoost is effective because it focuses the subsequent weak learners on the examples that are difficult to classify, thereby improving the overall performance of the ensemble. However, it's sensitive to noisy data and outliers, and it can be prone to overfitting if the weak learners are too complex.\n",
    "\n",
    "    # what is gradient boosting?\n",
    "    # Gradient Boosting is another popular ensemble learning technique used in machine learning, particularly for regression and classification tasks. It builds a strong learner by combining the predictions of multiple weak learners, typically decision trees, in a sequential manner.\n",
    "\n",
    "    Here's how gradient boosting generally works:\n",
    "\n",
    "    1. **Initialize Model**: A simple model (often a single leaf) is used to make an initial prediction for all data points. This initial prediction could be a simple value, such as the mean for regression tasks or the log odds for classification tasks.\n",
    "\n",
    "    2. **Compute Residuals**: The difference between the initial predictions and the actual target values (residuals) is computed. These residuals represent the errors made by the initial model.\n",
    "\n",
    "    3. **Train Weak Learner on Residuals**: A weak learner (often a decision tree) is trained to predict the residuals. The goal of this weak learner is to capture the remaining information in the data that the initial model failed to capture.\n",
    "\n",
    "    4. **Update Model**: The predictions of the weak learner are combined with the predictions of the initial model. This combination is typically done by adding the predictions, effectively moving the model closer to the true target values.\n",
    "\n",
    "    5. **Repeat Steps 2-4**: Steps 2-4 are repeated iteratively. In each iteration, a new weak learner is trained to predict the residuals of the current ensemble model. The predictions of all weak learners are then combined to update the ensemble model.\n",
    "\n",
    "    6. **Stopping Criteria**: The process continues until a predefined number of weak learners have been trained or until a certain level of performance is achieved.\n",
    "\n",
    "    7. **Final Prediction**: The final prediction is made by aggregating the predictions of all weak learners in the ensemble.\n",
    "\n",
    "    Gradient boosting is powerful because it sequentially improves the model by focusing on the residuals of the previous predictions. It can effectively capture complex relationships in the data and handle both regression and classification tasks. However, like AdaBoost, it can be sensitive to noisy data and overfitting if not properly tuned. Popular implementations of gradient boosting include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "    # what is XG boosting?\n",
    "    # XGBoost stands for eXtreme Gradient Boosting. It is an optimized and highly efficient implementation of the gradient boosting algorithm, designed for speed and performance. XGBoost is widely used in machine learning competitions and is also commonly used in various real-world applications due to its accuracy and scalability.\n",
    "\n",
    "    Here are some key features and characteristics of XGBoost:\n",
    "\n",
    "    1. **Tree Ensemble Method**: Like other gradient boosting implementations, XGBoost builds an ensemble of weak learners (typically decision trees) sequentially, where each new tree corrects the errors of the previous ones.\n",
    "\n",
    "    2. **Optimized Performance**: XGBoost is optimized for speed and efficiency. It is implemented in C++ and provides interfaces for various programming languages, including Python, R, Java, and others, which make it suitable for large-scale and high-performance applications.\n",
    "\n",
    "    3. **Regularization**: XGBoost provides several techniques for controlling overfitting, including L1 and L2 regularization terms added to the objective function, which penalize the complexity of the model.\n",
    "\n",
    "    4. **Handling Missing Values**: XGBoost has built-in support for handling missing values in the data, allowing it to automatically learn how to best handle missing data during the training process.\n",
    "\n",
    "    5. **Parallel and Distributed Computing**: XGBoost can leverage parallel and distributed computing capabilities to accelerate training on multicore CPUs or distributed computing frameworks like Apache Spark.\n",
    "\n",
    "    6. **Customization**: XGBoost provides a wide range of hyperparameters that can be tuned to optimize model performance, including parameters related to tree construction, learning rate, regularization, and more.\n",
    "\n",
    "    7. **Feature Importance**: XGBoost provides methods for assessing the importance of features in the model, which can help with feature selection and interpretation.\n",
    "\n",
    "    Overall, XGBoost is known for its accuracy, speed, and flexibility, making it a popular choice for a wide range of machine learning tasks, including classification, regression, ranking, and recommendation systems.\n",
    "\n",
    "    # what is SVM?\n",
    "    # Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It's particularly well-suited for classification of complex datasets with a clear margin of separation.\n",
    "\n",
    "    Here's how SVM works for classification:\n",
    "\n",
    "    1. **Identifying the Hyperplane**: Given a set of training data points, SVM finds the hyperplane that best separates the classes. The hyperplane is the decision boundary that maximizes the margin between the classes.\n",
    "\n",
    "    2. **Maximizing Margin**: SVM aims to find the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class. This ensures robust generalization to unseen data.\n",
    "\n",
    "    3. **Support Vectors**: The data points that lie closest to the hyperplane are called support vectors. These points are crucial in defining the hyperplane and determining the margin.\n",
    "\n",
    "    4. **Dealing with Non-Linearity**: SVM can handle non-linear classification tasks by using techniques like the kernel trick. The kernel trick maps the original feature space into a higher-dimensional space where the classes become linearly separable, allowing SVM to find a hyperplane in that space.\n",
    "\n",
    "    5. **Regularization**: SVM includes a regularization parameter (C) that balances the margin maximization and the classification error. A larger C value allows for fewer misclassifications but may lead to overfitting, while a smaller C value encourages a larger margin but may allow more misclassifications.\n",
    "\n",
    "    SVM has several advantages, including:\n",
    "\n",
    "    - Effective in high-dimensional spaces.\n",
    "    - Memory efficient as it uses only a subset of training points as support vectors.\n",
    "    - Versatile with different kernel functions for handling various data distributions.\n",
    "\n",
    "    However, SVM can be sensitive to the choice of kernel and parameters, and it might not perform well on very large datasets. Additionally, it's primarily designed for binary classification tasks, although it can be extended to handle multi-class classification.\n",
    "\n",
    "    # what is hyperplane?\n",
    "    # A hyperplane is a concept from geometry, particularly in the field of linear algebra. In n-dimensional space, a hyperplane is an n-1 dimensional subspace. In more intuitive terms:\n",
    "\n",
    "    In 1-dimensional space (a straight line), a hyperplane is just a point.\n",
    "    In 2-dimensional space (a plane), a hyperplane is a line.\n",
    "    In 3-dimensional space, a hyperplane is a flat 2-dimensional surface, like a sheet of paper.\n",
    "    In higher-dimensional spaces, it becomes harder to visualize, but it's essentially a flat subspace that divides the space into two half-spaces.\n",
    "\n",
    "    In linear algebra and machine learning, hyperplanes are often used in the context of classification tasks. For example, in a binary classification problem in 2D space, a hyperplane is a line that separates the space into two regions corresponding to the two classes. In higher dimensions, this generalizes to separating hyperplanes, which are used in algorithms like Support Vector Machines (SVMs) to classify data points.\n",
    "\n",
    "    # what is clustering & types of clustering?\n",
    "    # Clustering is a technique used in machine learning and data mining to group similar data points together based on certain characteristics or features. The goal of clustering is to partition a dataset into subsets, or clusters, such that data points within the same cluster are more similar to each other than to those in other clusters.\n",
    "\n",
    "    There are several types of clustering algorithms, each with its own approach to defining and identifying clusters:\n",
    "\n",
    "    1. **K-Means Clustering**: This is one of the most popular clustering algorithms. It partitions data into k clusters where each data point belongs to the cluster with the nearest mean. It works iteratively to assign each data point to one of k clusters based on the features provided.\n",
    "\n",
    "    2. **Hierarchical Clustering**: This type of clustering builds a hierarchy of clusters. It can be agglomerative, where each data point starts in its own cluster and clusters are recursively merged together, or divisive, where all data points start in one cluster and splits are recursively performed.\n",
    "\n",
    "    3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: DBSCAN is a density-based clustering algorithm that identifies clusters as areas of high density separated by areas of low density. It is particularly effective at identifying clusters of arbitrary shape and handling noise in the data.\n",
    "\n",
    "    4. **Mean Shift Clustering**: This algorithm is similar to K-means but does not require specifying the number of clusters in advance. It works by iteratively shifting data points towards the mode (the highest density of data points) in the feature space.\n",
    "\n",
    "    5. **Gaussian Mixture Models (GMM)**: GMM assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters. It models each cluster with a Gaussian distribution and estimates the parameters using the Expectation-Maximization (EM) algorithm.\n",
    "\n",
    "    6. **Spectral Clustering**: Spectral clustering techniques use the eigenvalues of a similarity matrix to reduce the dimensionality of the data before clustering in fewer dimensions. It is particularly useful when dealing with non-linear boundaries between clusters.\n",
    "\n",
    "    7. **Agglomerative Clustering**: This hierarchical clustering method starts with each data point as a single cluster and merges the closest pairs of clusters until only one cluster remains or a stopping criterion is met.\n",
    "\n",
    "    These are just a few examples of clustering algorithms, each suitable for different types of data and clustering tasks. The choice of algorithm depends on the nature of the data, the desired number of clusters, and computational constraints.\n",
    "\n",
    "    # what is principal component ananlysis?\n",
    "    # it is used to extract best feature from the multiple features for ananlysis\n",
    "\n",
    "    **Principal Component Analysis (PCA)** is a technique used for **dimensionality reduction** in data analysis and machine learning. Let me explain it in simple terms:\n",
    "\n",
    "    1. **Curse of Dimensionality**:\n",
    "       - As the number of features or dimensions in a dataset increases, the amount of data required to obtain statistically significant results also increases exponentially.\n",
    "       - This leads to issues like overfitting, increased computation time, and reduced accuracy of machine learning models.\n",
    "       - The problem arises because the number of possible combinations of features grows exponentially with dimensionality, making it computationally challenging to work with high-dimensional data.\n",
    "\n",
    "    2. **What is PCA?**:\n",
    "       - **Principal Component Analysis (PCA)** is a statistical procedure that aims to reduce the dimensionality of a dataset while preserving as much of the original information as possible.\n",
    "       - It works by finding a new set of variables (called **principal components**) that are uncorrelated and capture the maximum variance in the data.\n",
    "       - These principal components are linear combinations of the original features.\n",
    "       - PCA is widely used in exploratory data analysis and predictive modeling.\n",
    "\n",
    "    3. **How Does PCA Work?**:\n",
    "       - Given a dataset with multiple correlated variables, PCA identifies a set of orthogonal axes (principal components) that represent the most significant variations in the data.\n",
    "       - The first principal component captures the most variance, the second captures the second most, and so on.\n",
    "       - By projecting the data onto these principal components, we reduce the dimensionality while retaining essential patterns.\n",
    "       - The principal components are ordered by importance, so we can choose to keep only the top few components.\n",
    "\n",
    "    4. **Use Cases of PCA**:\n",
    "       - **Dimensionality Reduction**: PCA reduces the number of features, making subsequent computations more efficient.\n",
    "       - **Data Visualization**: It helps visualize high-dimensional data in lower dimensions.\n",
    "       - **Feature Selection**: By selecting the most important principal components, we can simplify models.\n",
    "       - **Data Compression**: PCA can compress data while preserving essential information.\n",
    "\n",
    "    In summary, PCA transforms the original features into a new coordinate system, emphasizing the directions of maximum variance. It's a powerful tool for handling high-dimensional data! 🌟\n",
    "\n",
    "    For a more detailed explanation and practical examples, you can explore resources like GeeksforGeeks¹, Wikipedia², or Built In³.\n",
    "\n",
    "    Source: Conversation with Bing, 2/27/2024\n",
    "    (1) Principal Component Analysis(PCA) - GeeksforGeeks. https://www.geeksforgeeks.org/principal-component-analysis-pca/.\n",
    "    (2) Principal component analysis - Wikipedia. https://en.wikipedia.org/wiki/Principal_component_analysis.\n",
    "    (3) Principal Component Analysis (PCA) Explained | Built In. https://builtin.com/data-science/step-step-explanation-principal-component-analysis.\n",
    "        \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # AI CONCEPTS :\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    import nltk\n",
    "\n",
    "    # NLTK  & techniques \n",
    "    # NLTK stands for Natural Language Toolkit. It's a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet. NLTK also includes a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\n",
    "    Some common techniques and functionalities in NLTK include:\n",
    "\n",
    "    1. **Tokenization**: Breaking text into words or sentences.\n",
    "    2. **Part-of-speech (POS) Tagging**: Assigning grammatical categories (like noun, verb, adjective) to words in a sentence.\n",
    "    3. **Named Entity Recognition (NER)**: Identifying proper nouns (such as names of people, organizations, locations) in text.\n",
    "    4. **Stemming and Lemmatization**: Reducing words to their base or root form.\n",
    "    5. **Parsing**: Analyzing the grammatical structure of sentences.\n",
    "    6. **WordNet Integration**: Accessing WordNet, a lexical database of English.\n",
    "    7. **Text Classification**: Categorizing text into predefined categories or classes.\n",
    "    8. **Sentiment Analysis**: Determining the sentiment (positive, negative, neutral) expressed in text.\n",
    "    9. **Corpus Management**: Accessing and manipulating text corpora for analysis.\n",
    "    10. **Language Modeling**: Building statistical models of language to predict or generate text.\n",
    "\n",
    "    These techniques can be used individually or in combination to perform various natural language processing tasks, such as text classification, sentiment analysis, information retrieval, machine translation, and more. NLTK provides a comprehensive set of tools and resources for researchers, educators, and practitioners in the field of computational linguistics and natural language processing.\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    # it is the natural language toolkit \n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Sure, let's break down each of these commands:\n",
    "\n",
    "    1. `nltk.download('punkt')`:\n",
    "       - This command is used to download the 'punkt' tokenizer models from NLTK's data repository.\n",
    "       - The 'punkt' tokenizer is a pre-trained model for tokenizing text into individual words or sentences. It's commonly used for tasks such as word tokenization and sentence tokenization.\n",
    "       - By downloading the 'punkt' tokenizer, you ensure that NLTK has access to the necessary resources to perform tokenization tasks effectively.\n",
    "\n",
    "    2. `nltk.download('wordnet')`:\n",
    "       - This command downloads the WordNet corpus from NLTK's data repository.\n",
    "       - WordNet is a lexical database of English that provides semantic relationships between words, such as synonyms, hypernyms, hyponyms, and meronyms.\n",
    "       - WordNet is commonly used in tasks such as word sense disambiguation, semantic similarity calculation, and building lexical resources for natural language processing applications.\n",
    "\n",
    "    3. `nltk.download('stopwords')`:\n",
    "       - This command downloads a list of stopwords from NLTK's data repository.\n",
    "       - Stopwords are common words that are often filtered out during text preprocessing because they typically do not carry significant meaning in a given context (e.g., \"the\", \"is\", \"and\", \"of\").\n",
    "       - NLTK's stopwords corpus contains lists of stopwords for various languages, which can be useful for tasks such as text normalization, feature extraction, and improving the performance of natural language processing algorithms by reducing noise.\n",
    "\n",
    "    Overall, these commands are used to download essential resources (tokenizer models, lexical databases, stopwords lists) from NLTK's data repository, enabling you to perform various text processing and natural language processing tasks effectively in Python using NLTK.\n",
    "\n",
    "\n",
    "    # it should be run only one time , no need to run again & again \n",
    "\n",
    "    [nltk_data] Downloading package punkt to C:\\Users\\Amrendra\n",
    "    [nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
    "    [nltk_data]   Package punkt is already up-to-date!\n",
    "    [nltk_data] Downloading package wordnet to C:\\Users\\Amrendra\n",
    "    [nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
    "    [nltk_data] Downloading package stopwords to C:\\Users\\Amrendra\n",
    "    [nltk_data]     Mishra\\AppData\\Roaming\\nltk_data...\n",
    "    [nltk_data]   Unzipping corpora\\stopwords.zip.\n",
    "\n",
    "Out\\[6\\]:\n",
    "\n",
    "    True\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    document=('my name is ankita and i am very good girl')\n",
    "\n",
    "    # here we have created the document \n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    document.split()\n",
    "\n",
    "    # applied split function on document for splitting words.\n",
    "\n",
    "Out\\[8\\]:\n",
    "\n",
    "    ['my', 'name', 'is', 'ankita', 'and', 'i', 'am', 'very', 'good', 'girl']\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # TOKENIZATION \n",
    "\n",
    "from nltk.tokenize import word_tokenize \\# The command \\`from\n",
    "nltk.tokenize import word_tokenize\\` is a Python import statement used\n",
    "to import a specific function called \\`word_tokenize\\` from the\n",
    "\\`tokenize\\` module within the \\`nltk\\` library. Here's what each part\n",
    "of the command means: - \\`from nltk.tokenize\\`: This part indicates that\n",
    "we're importing something from the \\`tokenize\\` module within the\n",
    "\\`nltk\\` library. \\`nltk\\` stands for Natural Language Toolkit, and it's\n",
    "a popular library in Python for working with human language data. -\n",
    "\\`import word_tokenize\\`: This part specifies that we want to import the\n",
    "\\`word_tokenize\\` function from the \\`tokenize\\` module.\n",
    "\\`word_tokenize\\` is a function provided by NLTK that tokenizes a piece\n",
    "of text (usually a sentence) into individual words, considering\n",
    "punctuation and other factors. So, when you execute \\`from nltk.tokenize\n",
    "import word_tokenize\\`, you're essentially making the \\`word_tokenize\\`\n",
    "function available for use in your Python script or interactive session.\n",
    "You can then call this function to tokenize text and obtain a list of\n",
    "words from a given sentence or document.\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    tokens = word_tokenize(document)\n",
    "\n",
    "    # The line `tokens = word_tokenize(document)` is a Python statement that tokenizes the text contained in the variable `document` using the `word_tokenize` function from the NLTK library. Here's a breakdown of what's happening:\n",
    "\n",
    "    - `document`: This variable likely contains a piece of text (e.g., a sentence, paragraph, or document) that you want to tokenize into individual words.\n",
    "    - `word_tokenize`: This is the function imported from NLTK that performs word tokenization. It takes a string of text as input and returns a list of words, where each word is a separate element in the list.\n",
    "    - `tokens`: This variable is used to store the result of tokenizing the text. After executing this line, `tokens` will contain a list of words extracted from the `document`, with each word as a separate element in the list.\n",
    "\n",
    "    So, essentially, `tokens = word_tokenize(document)` tokenizes the text stored in the variable `document` and stores the resulting list of words in the variable `tokens`, allowing you to further analyze or manipulate the individual words in the text.\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    tokens\n",
    "\n",
    "    # checked token \n",
    "\n",
    "Out\\[12\\]:\n",
    "\n",
    "    ['my', 'name', 'is', 'ankita', 'and', 'i', 'am', 'very', 'good', 'girl']\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "    len(tokens)\n",
    "\n",
    "    # checking length of tokens \n",
    "\n",
    "Out\\[13\\]:\n",
    "\n",
    "    10\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation \n",
    "\n",
    "    # The two lines:\n",
    "\n",
    "    ```python\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation\n",
    "    ```\n",
    "\n",
    "    are Python import statements used to import specific resources from the NLTK library and Python's built-in `string` module. Let's break down each line:\n",
    "\n",
    "    1. `from nltk.corpus import stopwords`:\n",
    "       - This line imports the `stopwords` corpus from the NLTK library.\n",
    "       - `stopwords` are common words (like \"the\", \"is\", \"and\") that are often filtered out from text during natural language processing tasks because they typically do not carry significant meaning.\n",
    "       - By importing the `stopwords` corpus from NLTK, you gain access to lists of stopwords in various languages, which you can use to filter out these words from your text data.\n",
    "\n",
    "    2. `from string import punctuation`:\n",
    "       - This line imports the `punctuation` constant from Python's built-in `string` module.\n",
    "       - `punctuation` is a string containing commonly used punctuation characters (e.g., period, comma, exclamation mark).\n",
    "       - Importing `punctuation` allows you to easily access these punctuation characters for tasks such as text preprocessing, tokenization, or filtering.\n",
    "\n",
    "    After executing these import statements, you'll have access to NLTK's stopwords corpus and Python's `punctuation` constant in your current Python session or script. You can then use these resources for text preprocessing, feature extraction, or other natural language processing tasks.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # stemming \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # what is stemming & why it is used for ?\n",
    "\n",
    "    # Stemming is a text normalization technique used in natural language processing to reduce words to their base or root form, known as the stem. The goal of stemming is to group together words with the same meaning but different inflections or derivations.\n",
    "\n",
    "    For example, consider the words \"running\", \"runs\", and \"runner\". These words are different forms of the same base word, \"run\". By stemming them, they would all be reduced to the common stem \"run\".\n",
    "\n",
    "    Stemming is used for several reasons:\n",
    "\n",
    "    1. **Text Normalization**: Stemming helps in standardizing text by reducing different variations of words to a common base form. This simplifies text processing and analysis.\n",
    "\n",
    "    2. **Information Retrieval**: In applications like search engines, stemming allows users to retrieve relevant documents even if they contain variations of the search query's keywords. For instance, a search for \"run\" could return documents containing \"running\" or \"ran\".\n",
    "\n",
    "    3. **Text Mining and Analysis**: Stemming is often used as a preprocessing step in text mining and analysis tasks, such as document clustering, topic modeling, and sentiment analysis. By reducing words to their stems, it helps in identifying common themes or patterns in text data.\n",
    "\n",
    "    4. **Improving Accuracy**: Stemming can sometimes improve the accuracy of text-based algorithms and models by reducing the vocabulary size and capturing the essence of the text more effectively. It can also help in reducing the sparsity of text representations, especially in cases where there is limited training data.\n",
    "\n",
    "    However, it's essential to note that stemming is a heuristic process and may not always produce accurate results. Stemmed words may not be valid words themselves, and there's a possibility of over-stemming or under-stemming, where words are incorrectly reduced or not reduced when they should be, respectively. Despite these limitations, stemming remains a widely used technique in natural language processing and text analysis tasks.\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "    words=[\"study\",\"studying\",\"studied\"]\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "    pip install nltk\n",
    "\n",
    "    # installed nltk by using pip command \n",
    "\n",
    "    Requirement already satisfied: nltk in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (3.8.1)\n",
    "    Requirement already satisfied: click in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
    "    Requirement already satisfied: joblib in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
    "    Requirement already satisfied: regex>=2021.8.3 in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
    "    Requirement already satisfied: tqdm in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
    "    Requirement already satisfied: colorama in c:\\users\\amrendra mishra\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
    "    Note: you may need to restart the kernel to use updated packages.\n",
    "\n",
    "In \\[31\\]:\n",
    "\n",
    "    from nltk import PorterStemmer\n",
    "\n",
    "    # The command `from nltk.stem import PorterStemmer` is used to import the `PorterStemmer` class from the `stem` module of the NLTK (Natural Language Toolkit) library. \n",
    "\n",
    "    Stemming is the process of reducing words to their root or base form, which helps in normalization and reducing the dimensionality of the feature space in natural language processing tasks like text classification, information retrieval, and more.\n",
    "\n",
    "    The `PorterStemmer` in NLTK implements the Porter stemming algorithm, which is one of the most commonly used algorithms for stemming English words. Once imported, you can create an instance of the `PorterStemmer` class and use its methods to stem words.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "     \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "     \n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
